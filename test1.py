#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@author: jmw418
"""
###Packages###
import numpy as np
import matplotlib.pyplot as plt
import scipy.linalg


### Initial condition function###
def jump(x, alpha=0.1, beta=0.3):
    """   
SUMMARY: 
     This is a function that produces a square wave height 1 in the range between
     alpha and beta it is 0 elsewhere
     
FLEXIBILITY:
     you can vary alpha and beta
     
INPUT:
    x     = the input of the jump function
    alpha = the left constant
    beta  = the right constant
    
OUTPUT: 
    1 if x belongs between alpha and beta and 0 else """
    one = lambda x: 1
    return np.where((x<beta) & (x>=alpha), one(x), 0.)
    
def cosBell(x, alpha=0.1, beta=0.3):
    """   
SUMMARY: 
     This is a function that produces a cos wave height 1 in the range between
     alpha and beta it is 0 elsewhere
     
FLEXIBILITY:
     you can vary alpha and beta
     
INPUT:
    x     = the input of the jump function
    alpha = the left constant
    beta  = the right constant
    
OUTPUT: 
    0.5*(1 - np.cos(2*np.pi*(x-alpha)/width)) if x belongs between alpha 
    and beta and 0 else """
    width = beta - alpha
    bell = lambda x: 0.5*(1 - np.cos(2*np.pi*(x-alpha)/width))
    return np.where((x<beta) & (x>=alpha), bell(x), 0.)


###Timestepping methods###
def picard(X, nx, nt, Tfinal, mu, theta, form):
    """
SUMMARY:
    This is a timestepping algorithm that solves the burgers equation \
    when it is discretised by the picard method.

FLEXIBILITY:
    The flexibility is not in this function, it is in the SolveBurger
    
INPUT: X generated by the initial condition
    
OUTPUT: X generated by the timestepping method
    """
    ### Parameters ###
    dx = (1-0)/(nx-1) 
    dt = (Tfinal-0)/(nt-1)
    C = 1*dt/(2*dx)
    D = mu*dt/(dx**2)
    
    ### warnings ###
    ### Initialise structure ###
    beta = np.zeros([nx])
    A = np.zeros([nx,nx])
    if form == "non conservative":
        if theta > 0: ###we need to invert a matrix 
            for i in range(0,nt-1):###time stepping###
        
                for j in range(0,nx): ### create vector on RHS###
                    beta[j] = X[j]\
                    - C*(1-theta)*X[j]*(X[(j+1)%nx] - X[(j-1)%nx])\
                    + (D)*(1-theta)*(X[(j-1)%nx]-2*X[j]+X[(j+1)%nx])
        
                for p in range(0,nx): ### Create Matrix on LHS ###
                    A[p,(p+1)%nx] =  theta*C*X[p] - theta*D # b 
                    A[p,(p-1)%nx] = -theta*C*X[p] - theta*D  #c  
                    A[p,p] = 1 + 2*theta*D
            
                X[:] = scipy.linalg.solve(A, beta)### Solving for next timestep ###
        if theta == 0: ## to avoid inverting a identity matrix pointlessly.
            for i in range(0,nt-1):###time stepping###
                for j in range(0,nx): ### create vector on RHS###
                    beta[j] = X[j]\
                    - C*X[j]*(X[(j+1)%nx] - X[(j-1)%nx])\
                    + D*(X[(j-1)%nx]-2*X[j]+X[(j+1)%nx])
    
                X[:] = beta ### Solving for next timestep ###

    if form == "conservative":
        if theta >0:
            for i in range(0,nt-1):###time stepping###
        
                for j in range(0,nx): ### create vector on RHS###
                    beta[j] = X[j]\
                    - C*0.5*(1)*(X[(j+1)%nx]**2 - X[(j-1)%nx]**2)\
                    + (D)*(1)*(X[(j-1)%nx]-2*X[j]+X[(j+1)%nx])
           
                for p in range(0,nx): ### Create Matrix on LHS ###
                    A[p,(p+1)%nx] =  0.5*theta*C*X[p] - theta*D # b 
                    A[p,(p-1)%nx] = -0.5*theta*C*X[p] - theta*D  #c  
                    A[p,p] = 1 + 2*theta*D + C*theta*0.5*(X[(p+1)%nx] - X[(p-1)%nx])
        
                X[:] = scipy.linalg.solve(A, beta)### Solving for next timestep ###
        ###
        if theta == 0: ## to avoid inverting a identity matrix pointlessly.
            for i in range(0,nt-1):###time stepping###
                for j in range(0,nx): ### create vector on RHS###
                    beta[j] = X[j]\
                    - C*0.5*(X[(j+1)%nx]**2 - X[(j-1)%nx]**2)\
                    + D*(X[(j-1)%nx]-2*X[j]+X[(j+1)%nx])
    
                X[:] = beta ### Solving for next timestep ###        
 
    return X

   
def newton(X, nx, nt, Tfinal, mu, theta, form):

    ### derived parameters ###
    dx = (1-0)/(nx-1) 
    dt = (Tfinal-0)/(nt-1)
    C = 1*dt/(2*dx)
    D = mu*dt/(dx**2)
    
    ##Creation of newton method structure 
    beta = np.zeros([nx])
    w = np.zeros([nx])
    dw = np.zeros([nx])
    if form == "non conservative":
        for i in range(0,nt-1):### time loop 
        

                                    ### first initialise w^0 = phi^n as initial guess###
            w[:]=X[:]              ### this is the starting guess for newton method ###
        
            ### Construct the newton loop
            tol = 10**(-13)## extreeme accuracy can be imposed
            err  = 2*tol
            while (err>tol): ### until convergence ###
            
                ### Create beta^k_j ###
                for q in range(0,nx): 
                    beta[q] = X[q] - C*(1-theta)*X[q]*(X[(q+1)%nx] - X[(q-1)%nx]) \
                    + (D)*(1-theta)*(X[(q-1)%nx]-2*X[q]+X[(q+1)%nx]) \
                    -w[q] - C*theta*w[q]*(w[(q+1)%nx]-w[(q-1)%nx]) + D*theta*(w[(q+1)%nx]-2*w[q] +w[(q-1)%nx])
            
                ##Create A^k_j ###
                A = np.zeros([nx,nx])
                for p in range(1,nx):
                    A[p-1,p] = ((theta*C*w[p])-(theta*D)) # b
                for p in range(0,nx-1):
                    A[p+1,p] = ((-theta*C*w[p+1])-(theta*D)) # c
                for p in range(0,nx):
                    A[p,p] = (1+ 2*theta*D + C*theta*(w[(p+1)%nx]-w[(p-1)%nx]))
            
            
                ## solving for dw
                dw = scipy.linalg.solve(A, beta)
                err = np.linalg.norm(dw,2)
                ## improve the newton loop
                w = w + dw 
                ## we have w^k
                if err > 10**6:
                    print("error = %a"%err)
                    
    
        
            for j in range(0,nx): ##replace by X[:] = w[:]
                X[j] = w[j]
    if form == "conservative":
        for i in range(0,nt-1):### time loop 
            ## first initialise w^0 = phi^n
            for j in range(0,nx): 
                w[j] = X[j]

            ### Construct the newton loop
            tol = 10**(-13) ### accuracy can be imposed ###
            err  = 2*tol
            while (err>tol): ## eventually replace with while loop and stopping criterion

                ### Create beta^k ###
                for q in range(0,nx): 
                    beta[q] = X[q] - C*0.5*(1-theta)*(X[(q+1)%nx]**2 - X[(q-1)%nx]**2) \
                    + (D)*(1-theta)*(X[(q-1)%nx]-2*X[q]+X[(q+1)%nx]) \
                    -w[q] - C*0.5*theta*(w[(q+1)%nx]**2-w[(q-1)%nx]**2) \
                    + D*theta*(w[(q+1)%nx]-2*w[q] +w[(q-1)%nx])
            
                ### Create A^k_j, tridiag ###
                A = np.zeros([nx,nx])
                for p in range(1,nx):
                    A[p-1,p] = ((theta*C*w[(p+1)%nx])-(theta*D)) # down zero across 1 = b
                for p in range(0,nx-1):
                    A[p+1,p] = ((-theta*C*w[(p-1)%nx])-(theta*D)) # down 1 acros 0 = c
                for p in range(0,nx):
                    A[p,p] = (1+ 2*theta*D + C*theta*(w[(p+1)%nx]-w[(p-1)%nx]))
        
                ## solving for dw
                dw = scipy.linalg.solve(A, beta)
                err = np.linalg.norm(dw,2)
                ## improve the newton loop
                w = w + dw 
                ## we have w^k
                if err > 10**6:
                    print("error = %a"%err)
                
            for j in range(0,nx):
                X[j] = w[j]

    return X 

def SolveBurger(nx ,nt ,Tfinal ,mu, method, theta, form, IC):
    """
SUMMARY:
    This function takes an "initial_condition" and solves
    the "conservative" or "non conservative" burgers equation 
    numerically for some later time "Tfinal".
                    
FLEXIBILITY:
    You can choose:  the spatial discretisation used "nx", the number of timesteps "nt",
    the viscosity term "mu", the "method" used, the implicitness of the scheme
    "theta"   
      
INPUT:      
    nx      = number of space points
    nt      = number of time points 
    Tfinal  = the final time  
    mu      = the viscosity in the burgers equation 
    method  = "picard" SOLVES APPROXIMATION TO BURGERS EQUATION
              "newton" SOLVES EXACT BURGERS EQUATION
    theta   = 1 is implicit,
              0 is explicit,
              You can put in other real values, 
    form    = "conservative"     -->    u_t + (u^2/2)_x - mu u_{xx} = 0 
              "non conservative" -->    u_t + u(u_x) - mu u_{xx} = 0 
Output:
    A vector representing the numerical solution 
                """
    
    ##**Creating the structure**## 
    X = np.zeros([nx])
    x = np.linspace(0,1,nx)
    ### Initialisation of Initial conditions, as a d vector###
    if IC == "jump":
        X[:] = jump(x, 0.1, 0.3)
    if IC == "cosBell":
        X[:] = cosBell(x,0.1,0.3)
    
    ### Timestepping methods ###
    if method == "newton":
        newton(X, nx, nt, Tfinal, mu, theta, form)
    if method == "picard":
        picard(X, nx, nt, Tfinal, mu, theta, form)
    return X




def __main__():

    ###----- Test: Demonstration of inacuracy of picards method -----###
    ### we have theta = 0 and now the method isnt unconditionally stable ###
    ### we will explore 2 cases

  
    
    ###high accuracy solutions###
    nx = 401
    nt = 1001
    x = np.linspace(0,1,nx) 
    plt.figure(figsize= (9,5)) 
    X = SolveBurger(nx,nt,1,0.001,"picard",0,"non conservative","jump")
    plt.plot(x, X, 'r',linewidth=0.5,label = "t = 1, ftcs")
    nx = 401
    nt = 1001
    x = np.linspace(0,1,nx) 
    X = SolveBurger(nx,nt,1,0.001,"newton",1,"non conservative","jump")
    plt.plot(x, X, color = 'b' ,linewidth=0.5,label = "newt")
    nx = 401
    nt = 1001
    x = np.linspace(0,1,nx) 
    X = SolveBurger(nx,nt,1,0.001,"picard",0,"conservative","jump")
    plt.plot(x, X, 'k',linewidth=0.5,label = "t = 1, ftcs-cons")
    nx = 401
    nt = 1001
    x = np.linspace(0,1,nx) 
    X = SolveBurger(nx,nt,1,0.001,"newton",1,"conservative","jump")
    plt.plot(x, X, color = 'g' ,linewidth=0.5,label = "newt-cons")
    plt.ylabel('$\phi$')
    plt.xlabel('x')
    plt.legend()
    plt.show()
    
    
    
    
    
    
    
    
        ###----- Test: Demonstration of inacuracy of picards method -----###
    ### we have theta =1 and now the method is unconditionally stable ###
    nx = 201
    x = np.linspace(0,1,nx) 
    plt.figure(figsize= (9,5))
    X = SolveBurger(201,15,0,0.001,"picard",1,"non conservative","jump")
    plt.plot(x, X, 'k',linewidth=0.5,label = "initial conditions")
    X = SolveBurger(201,15,1,0.001,"picard",1,"non conservative","jump")
    plt.plot(x, X, 'r',linewidth=0.5,label = "t = 1, nt =15")
    X = SolveBurger(201,101,1,0.001,"picard",1,"non conservative","jump")
    plt.plot(x, X, 'b',linewidth=0.5,label = "t = 1, nt =101")
    X = SolveBurger(201,201,1,0.001,"picard",1,"non conservative","jump")
    plt.plot(x, X, 'g',linewidth=0.5,label = "t = 1, nt =201")
    X = SolveBurger(201,1001,1,0.001,"picard",1,"non conservative","jump")
    plt.plot(x, X, 'b',linewidth=0.5,label = "t = 1, nt =1001")
    plt.ylabel('$\phi$')
    plt.xlabel('x')
    plt.legend()
    plt.show()
      
    ###Conclusion: picards method will always converge, however the equation
    ### you are solving is only accurate for small timestepping, however the 

    ###-----  Test: Timestep restrictions with newton method advective form -----###
    ntf = np.zeros([4])
    ntf[:] = [25,42,70,74] #160## # smallest nt available ie largest timesteps possible
    plt.figure(figsize= (9,5))
    for j in range(0,4):
        nx = 25*(2**j) + 1
        nt = int(ntf[j])
    
        ### number of spatial points space size \Delta{x} = 0.01
        x = np.linspace(0,1,nx) 
        ## here I test fully time implicit (theta =1)
        ## mu = 0.001, Tfinal = 1, nt = 70
        
        X = SolveBurger(nx,nt,0,0.001,"newton",1,"non conservative","jump")
        plt.plot(x, X, 'k',linewidth=0.5)
        X = SolveBurger(nx,nt,1,0.001,"newton",1,"non conservative","jump")
        plt.plot(x, X, color = (0,0.2+0.2*j,1-0.2*j) ,linewidth=0.5,label = "nx = %a ,nt = %a " %(nx,nt))
        plt.ylabel('$\phi$')
        plt.xlabel('x')
        plt.legend()
        plt.title("Fully Implicit Newton Method on Advective Form of Burgers")
    plt.show()
  
     ###-----  Test: Timestep restrictions with newton method conservativ form -----###
    ntf = np.zeros([4])
    ntf[:] = [34,70,127,237] # smallest nt available ie largest timesteps possible
    plt.figure(figsize= (9,5))
    for j in range(0,4):
        nx = 25*(2**j) + 1
        nt = int(ntf[j])
    
        ### number of spatial points space size \Delta{x} = 0.01
        x = np.linspace(0,1,nx) 
        ## here I test fully time implicit (theta =1)
        ## mu = 0.001, Tfinal = 1, nt = 70
        
        X = SolveBurger(nx,nt,0,0.001,"newton",1,"conservative","jump")
        plt.plot(x, X, 'k',linewidth=0.5)
        X = SolveBurger(nx,nt,1,0.001,"newton",1,"conservative","jump")
        plt.plot(x, X, color = (1-0.2*j,0.2+0.2*j,0.0) ,linewidth=0.5,label = "nx = %a ,nt = %a " %(nx,nt))
        plt.ylabel('$\phi$')
        plt.xlabel('x')
        plt.legend()
        plt.title("Fully Implicit Newton Method on Conservative Form of Burgers")
    plt.show()
    

  

__main__()